{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "asl_transfer_learning_evaluation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aniketsharma00411/sign-language-to-text-translator/blob/main/transfer_learning_evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2SPnZg5D_kx"
      },
      "source": [
        "# Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9Xgk6PSD_ky"
      },
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import applications\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "ZiMkfWayD_k0",
        "outputId": "8d9599af-910a-464a-a2fb-99688bf80101"
      },
      "source": [
        "if not os.path.exists(os.path.expanduser('~')+'/.kaggle'):\n",
        "    ! mkdir ~/.kaggle\n",
        "os.chdir(os.path.expanduser('~')+'/.kaggle')\n",
        "if not os.path.exists(os.path.expanduser('~')+'/.kaggle/kaggle.json'):\n",
        "    kaggle_api_file = files.upload()\n",
        "    ! kaggle datasets download -d grassknoted/asl-alphabet\n",
        "    ! unzip -q asl-alphabet.zip\n",
        "    ! rm -rf asl_alphabet_train/asl_alphabet_train/del"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-6e99cf55-e9c4-4c73-9b32-5d832fdd1040\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-6e99cf55-e9c4-4c73-9b32-5d832fdd1040\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "Downloading asl-alphabet.zip to /root/.kaggle\n",
            " 99% 1.01G/1.03G [00:11<00:00, 106MB/s] \n",
            "100% 1.03G/1.03G [00:11<00:00, 92.0MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8dgYLreFq_w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06c34c64-6235-4f0b-c14f-c2922779477e"
      },
      "source": [
        "! ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "asl_alphabet_test  asl_alphabet_train  asl-alphabet.zip  kaggle.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sytb1W-rzd2"
      },
      "source": [
        "# Creating Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDN0CdRz4rDR"
      },
      "source": [
        "def get_data(preprocessing_function):\n",
        "    image_gen = ImageDataGenerator(preprocessing_function=preprocessing_function,\n",
        "                                   validation_split=0.2)\n",
        "\n",
        "    train_data = 'asl_alphabet_train/asl_alphabet_train'\n",
        "\n",
        "    train_gen = image_gen.flow_from_directory(train_data,\n",
        "                                              target_size=(224, 224),\n",
        "                                              class_mode='categorical',\n",
        "                                              color_mode='rgb',\n",
        "                                              shuffle=True,\n",
        "                                              batch_size=128,\n",
        "                                              seed=0,\n",
        "                                              subset='training')\n",
        "\n",
        "    val_gen = image_gen.flow_from_directory(train_data,\n",
        "                                            target_size=(224, 224),\n",
        "                                            class_mode='categorical',\n",
        "                                            color_mode='rgb',\n",
        "                                            shuffle=True,\n",
        "                                            batch_size=128,\n",
        "                                            seed=0,\n",
        "                                            subset='validation')\n",
        "\n",
        "    return train_gen, val_gen"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvum5L5QD_k7"
      },
      "source": [
        "def get_model(model):\n",
        "    kwargs =    {'include_top':False,\n",
        "                 'weights':'imagenet',\n",
        "                 'input_shape':(224, 224, 3),\n",
        "                 'pooling':'avg'}\n",
        "    \n",
        "    base_model = model(**kwargs)\n",
        "    \n",
        "    end_model = models.Sequential()\n",
        "    end_model.add(layers.Flatten(input_shape=base_model.output_shape[1:]))\n",
        "    end_model.add(layers.Dense(64))\n",
        "    end_model.add(layers.LeakyReLU())\n",
        "    end_model.add(layers.Dense(64))\n",
        "    end_model.add(layers.LeakyReLU())\n",
        "    end_model.add(layers.Dense(28, activation='softmax'))\n",
        "\n",
        "    model = models.Model(inputs=base_model.input, outputs=end_model(base_model.output))\n",
        "\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSE1wHcbD_k7"
      },
      "source": [
        "models_dict = {\n",
        "    'Xception': {'model': applications.xception.Xception,\n",
        "                 'preprocess_func': applications.xception.preprocess_input},\n",
        "    'VGG16': {'model': applications.vgg16.VGG16,\n",
        "              'preprocess_func': applications.vgg16.preprocess_input},\n",
        "    'VGG19': {'model': applications.vgg19.VGG19,\n",
        "              'preprocess_func': applications.vgg19.preprocess_input},\n",
        "    'ResNet50': {'model': applications.resnet.ResNet50,\n",
        "                 'preprocess_func': applications.resnet.preprocess_input},\n",
        "    'ResNet101': {'model': applications.resnet.ResNet101,\n",
        "                  'preprocess_func': applications.resnet.preprocess_input},\n",
        "    'ResNet152': {'model': applications.resnet.ResNet152,\n",
        "                  'preprocess_func': applications.resnet.preprocess_input},\n",
        "    'ResNet50V2': {'model': applications.resnet_v2.ResNet50V2,\n",
        "                   'preprocess_func': applications.resnet_v2.preprocess_input},\n",
        "    'ResNet101V2': {'model': applications.resnet_v2.ResNet101V2,\n",
        "                    'preprocess_func': applications.resnet_v2.preprocess_input},\n",
        "    'ResNet152V2': {'model': applications.resnet_v2.ResNet152V2,\n",
        "                    'preprocess_func': applications.resnet_v2.preprocess_input},\n",
        "    'InceptionV3': {'model': applications.inception_v3.InceptionV3,\n",
        "                    'preprocess_func': applications.inception_v3.preprocess_input},\n",
        "    'InceptionResNetV2': {'model': applications.inception_resnet_v2.InceptionResNetV2,\n",
        "                          'preprocess_func': applications.inception_resnet_v2.preprocess_input},\n",
        "    'MobileNet': {'model': applications.mobilenet.MobileNet,\n",
        "                  'preprocess_func': applications.mobilenet.preprocess_input},\n",
        "    'MobileNetV2': {'model': applications.mobilenet_v2.MobileNetV2,\n",
        "                    'preprocess_func': applications.mobilenet_v2.preprocess_input},\n",
        "    'DenseNet121': {'model': applications.densenet.DenseNet121,\n",
        "                    'preprocess_func': applications.densenet.preprocess_input},\n",
        "    'DenseNet169': {'model': applications.densenet.DenseNet169,\n",
        "                    'preprocess_func': applications.densenet.preprocess_input},\n",
        "    'DenseNet201': {'model': applications.densenet.DenseNet201,\n",
        "                    'preprocess_func': applications.densenet.preprocess_input},\n",
        "    'NASNetMobile': {'model': applications.nasnet.NASNetMobile,\n",
        "                     'preprocess_func': applications.nasnet.preprocess_input},\n",
        "    'EfficientNetB0': {'model': applications.efficientnet.EfficientNetB0,\n",
        "                       'preprocess_func': applications.efficientnet.preprocess_input},\n",
        "    'EfficientNetB1': {'model': applications.efficientnet.EfficientNetB1,\n",
        "                       'preprocess_func': applications.efficientnet.preprocess_input},\n",
        "    'EfficientNetB2': {'model': applications.efficientnet.EfficientNetB2,\n",
        "                       'preprocess_func': applications.efficientnet.preprocess_input},\n",
        "    'EfficientNetB3': {'model': applications.efficientnet.EfficientNetB3,\n",
        "                       'preprocess_func': applications.efficientnet.preprocess_input},\n",
        "    'EfficientNetB4': {'model': applications.efficientnet.EfficientNetB4,\n",
        "                       'preprocess_func': applications.efficientnet.preprocess_input},\n",
        "    'EfficientNetB5': {'model': applications.efficientnet.EfficientNetB5,\n",
        "                       'preprocess_func': applications.efficientnet.preprocess_input},\n",
        "    'EfficientNetB6': {'model': applications.efficientnet.EfficientNetB6,\n",
        "                       'preprocess_func': applications.efficientnet.preprocess_input},\n",
        "    'EfficientNetB7': {'model': applications.efficientnet.EfficientNetB7,\n",
        "                       'preprocess_func': applications.efficientnet.preprocess_input}\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3zxUdPJD_k7"
      },
      "source": [
        "# Training Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6f2HJeZqtIK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d1e9c24-ffb6-4e39-c5db-f0c88c14d6a9"
      },
      "source": [
        "for name, model in models_dict.items():\n",
        "    print(name)\n",
        "\n",
        "    train_gen, val_gen = get_data(model['preprocess_func'])\n",
        "    m = get_model(model['model'])\n",
        "\n",
        "    start = time.perf_counter()\n",
        "\n",
        "    history = m.fit(train_gen,\n",
        "                    epochs=1,\n",
        "                    validation_data=val_gen)\n",
        "\n",
        "    duration = time.perf_counter() - start\n",
        "\n",
        "    models_dict[name]['time'] = duration\n",
        "    models_dict[name]['val_acc'] = history.history['val_accuracy'][-1]\n",
        "    \n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Xception\n",
            "Found 67200 images belonging to 28 classes.\n",
            "Found 16800 images belonging to 28 classes.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "83689472/83683744 [==============================] - 0s 0us/step\n",
            "525/525 [==============================] - 398s 691ms/step - loss: 0.4918 - accuracy: 0.8765 - val_loss: 0.6463 - val_accuracy: 0.8014\n",
            "\n",
            "VGG16\n",
            "Found 67200 images belonging to 28 classes.\n",
            "Found 16800 images belonging to 28 classes.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 0s 0us/step\n",
            "525/525 [==============================] - 395s 712ms/step - loss: 0.4815 - accuracy: 0.8770 - val_loss: 0.5516 - val_accuracy: 0.8303\n",
            "\n",
            "VGG19\n",
            "Found 67200 images belonging to 28 classes.\n",
            "Found 16800 images belonging to 28 classes.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "80142336/80134624 [==============================] - 1s 0us/step\n",
            "525/525 [==============================] - 446s 848ms/step - loss: 0.4712 - accuracy: 0.8769 - val_loss: 0.6844 - val_accuracy: 0.7966\n",
            "\n",
            "ResNet50\n",
            "Found 67200 images belonging to 28 classes.\n",
            "Found 16800 images belonging to 28 classes.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94773248/94765736 [==============================] - 1s 0us/step\n",
            "525/525 [==============================] - 283s 530ms/step - loss: 0.3167 - accuracy: 0.9214 - val_loss: 0.6069 - val_accuracy: 0.8390\n",
            "\n",
            "ResNet101\n",
            "Found 67200 images belonging to 28 classes.\n",
            "Found 16800 images belonging to 28 classes.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet101_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "171450368/171446536 [==============================] - 3s 0us/step\n",
            "525/525 [==============================] - 475s 895ms/step - loss: 0.3047 - accuracy: 0.9218 - val_loss: 0.5934 - val_accuracy: 0.8398\n",
            "\n",
            "ResNet152\n",
            "Found 67200 images belonging to 28 classes.\n",
            "Found 16800 images belonging to 28 classes.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet152_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "234700800/234698864 [==============================] - 2s 0us/step\n",
            "525/525 [==============================] - 687s 1s/step - loss: 0.2812 - accuracy: 0.9290 - val_loss: 0.4135 - val_accuracy: 0.8815\n",
            "\n",
            "ResNet50V2\n",
            "Found 67200 images belonging to 28 classes.\n",
            "Found 16800 images belonging to 28 classes.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94674944/94668760 [==============================] - 1s 0us/step\n",
            "525/525 [==============================] - 241s 451ms/step - loss: 0.3103 - accuracy: 0.9203 - val_loss: 0.6038 - val_accuracy: 0.8374\n",
            "\n",
            "ResNet101V2\n",
            "Found 67200 images belonging to 28 classes.\n",
            "Found 16800 images belonging to 28 classes.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet101v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "171319296/171317808 [==============================] - 2s 0us/step\n",
            "525/525 [==============================] - 418s 786ms/step - loss: 0.2934 - accuracy: 0.9207 - val_loss: 0.6373 - val_accuracy: 0.8199\n",
            "\n",
            "ResNet152V2\n",
            "Found 67200 images belonging to 28 classes.\n",
            "Found 16800 images belonging to 28 classes.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet152v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "234553344/234545216 [==============================] - 3s 0us/step\n",
            "525/525 [==============================] - 605s 1s/step - loss: 0.3389 - accuracy: 0.9121 - val_loss: 0.6036 - val_accuracy: 0.8377\n",
            "\n",
            "InceptionV3\n",
            "Found 67200 images belonging to 28 classes.\n",
            "Found 16800 images belonging to 28 classes.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87916544/87910968 [==============================] - 0s 0us/step\n",
            "525/525 [==============================] - 192s 351ms/step - loss: 0.4950 - accuracy: 0.8629 - val_loss: 0.8852 - val_accuracy: 0.7633\n",
            "\n",
            "InceptionResNetV2\n",
            "Found 67200 images belonging to 28 classes.\n",
            "Found 16800 images belonging to 28 classes.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_resnet_v2/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "219062272/219055592 [==============================] - 1s 0us/step\n",
            "525/525 [==============================] - 453s 841ms/step - loss: 0.7006 - accuracy: 0.8024 - val_loss: 0.8248 - val_accuracy: 0.7511\n",
            "\n",
            "MobileNet\n",
            "Found 67200 images belonging to 28 classes.\n",
            "Found 16800 images belonging to 28 classes.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet/mobilenet_1_0_224_tf_no_top.h5\n",
            "17227776/17225924 [==============================] - 0s 0us/step\n",
            "525/525 [==============================] - 160s 300ms/step - loss: 0.3226 - accuracy: 0.9233 - val_loss: 0.3939 - val_accuracy: 0.8799\n",
            "\n",
            "MobileNetV2\n",
            "Found 67200 images belonging to 28 classes.\n",
            "Found 16800 images belonging to 28 classes.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "9412608/9406464 [==============================] - 0s 0us/step\n",
            "525/525 [==============================] - 167s 310ms/step - loss: 0.3607 - accuracy: 0.9097 - val_loss: 0.4593 - val_accuracy: 0.8598\n",
            "\n",
            "DenseNet121\n",
            "Found 67200 images belonging to 28 classes.\n",
            "Found 16800 images belonging to 28 classes.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "29089792/29084464 [==============================] - 0s 0us/step\n",
            "525/525 [==============================] - 264s 486ms/step - loss: 0.5286 - accuracy: 0.8667 - val_loss: 0.6431 - val_accuracy: 0.8114\n",
            "\n",
            "DenseNet169\n",
            "Found 67200 images belonging to 28 classes.\n",
            "Found 16800 images belonging to 28 classes.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet169_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "51879936/51877672 [==============================] - 1s 0us/step\n",
            "525/525 [==============================] - 307s 566ms/step - loss: 0.4202 - accuracy: 0.8998 - val_loss: 0.3918 - val_accuracy: 0.8845\n",
            "\n",
            "DenseNet201\n",
            "Found 67200 images belonging to 28 classes.\n",
            "Found 16800 images belonging to 28 classes.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet201_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "74842112/74836368 [==============================] - 1s 0us/step\n",
            "525/525 [==============================] - 398s 735ms/step - loss: 0.3657 - accuracy: 0.9142 - val_loss: 0.3801 - val_accuracy: 0.8873\n",
            "\n",
            "NASNetMobile\n",
            "Found 67200 images belonging to 28 classes.\n",
            "Found 16800 images belonging to 28 classes.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/nasnet/NASNet-mobile-no-top.h5\n",
            "19996672/19993432 [==============================] - 0s 0us/step\n",
            "525/525 [==============================] - 202s 360ms/step - loss: 0.6134 - accuracy: 0.8346 - val_loss: 0.8903 - val_accuracy: 0.7307\n",
            "\n",
            "EfficientNetB0\n",
            "Found 67200 images belonging to 28 classes.\n",
            "Found 16800 images belonging to 28 classes.\n",
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
            "16711680/16705208 [==============================] - 0s 0us/step\n",
            "525/525 [==============================] - 177s 327ms/step - loss: 0.3275 - accuracy: 0.9261 - val_loss: 0.3432 - val_accuracy: 0.8901\n",
            "\n",
            "EfficientNetB1\n",
            "Found 67200 images belonging to 28 classes.\n",
            "Found 16800 images belonging to 28 classes.\n",
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb1_notop.h5\n",
            "27025408/27018416 [==============================] - 0s 0us/step\n",
            "525/525 [==============================] - 225s 413ms/step - loss: 0.4353 - accuracy: 0.8918 - val_loss: 0.4090 - val_accuracy: 0.8793\n",
            "\n",
            "EfficientNetB2\n",
            "Found 67200 images belonging to 28 classes.\n",
            "Found 16800 images belonging to 28 classes.\n",
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb2_notop.h5\n",
            "31793152/31790344 [==============================] - 0s 0us/step\n",
            "525/525 [==============================] - 235s 432ms/step - loss: 0.4810 - accuracy: 0.8767 - val_loss: 0.4503 - val_accuracy: 0.8690\n",
            "\n",
            "EfficientNetB3\n",
            "Found 67200 images belonging to 28 classes.\n",
            "Found 16800 images belonging to 28 classes.\n",
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb3_notop.h5\n",
            "43941888/43941136 [==============================] - 0s 0us/step\n",
            "525/525 [==============================] - 304s 559ms/step - loss: 0.3828 - accuracy: 0.9048 - val_loss: 0.4232 - val_accuracy: 0.8808\n",
            "\n",
            "EfficientNetB4\n",
            "Found 67200 images belonging to 28 classes.\n",
            "Found 16800 images belonging to 28 classes.\n",
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb4_notop.h5\n",
            "71688192/71686520 [==============================] - 0s 0us/step\n",
            "525/525 [==============================] - 403s 745ms/step - loss: 0.4539 - accuracy: 0.8809 - val_loss: 0.4530 - val_accuracy: 0.8778\n",
            "\n",
            "EfficientNetB5\n",
            "Found 67200 images belonging to 28 classes.\n",
            "Found 16800 images belonging to 28 classes.\n",
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb5_notop.h5\n",
            "115269632/115263384 [==============================] - 1s 0us/step\n",
            "525/525 [==============================] - 552s 1s/step - loss: 0.3720 - accuracy: 0.9100 - val_loss: 0.4171 - val_accuracy: 0.8863\n",
            "\n",
            "EfficientNetB6\n",
            "Found 67200 images belonging to 28 classes.\n",
            "Found 16800 images belonging to 28 classes.\n",
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb6_notop.h5\n",
            "165240832/165234480 [==============================] - 1s 0us/step\n",
            "525/525 [==============================] - 713s 1s/step - loss: 0.5047 - accuracy: 0.8694 - val_loss: 0.5769 - val_accuracy: 0.8329\n",
            "\n",
            "EfficientNetB7\n",
            "Found 67200 images belonging to 28 classes.\n",
            "Found 16800 images belonging to 28 classes.\n",
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb7_notop.h5\n",
            "258080768/258076736 [==============================] - 3s 0us/step\n",
            "525/525 [==============================] - 958s 2s/step - loss: 0.3614 - accuracy: 0.9090 - val_loss: 0.4238 - val_accuracy: 0.8823\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3INO9J8r8u_"
      },
      "source": [
        "# Evaluating and Visualizing results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gga43tnFD_k8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 834
        },
        "outputId": "a0613946-048d-4c7c-d103-6ac525722ada"
      },
      "source": [
        "model_results = []\n",
        "\n",
        "for name, _ in models_dict.items():\n",
        "    model_results.append([name,\n",
        "                          models_dict[name]['val_acc'],\n",
        "                          models_dict[name]['time']])\n",
        "    \n",
        "results = pd.DataFrame(model_results,\n",
        "                       columns = ['Model', 'Validation Accuracy', 'Training Time (sec.)'])\n",
        "results = results.sort_values(by='Validation Accuracy', ascending=False).reset_index(drop=True)\n",
        "\n",
        "results"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Validation Accuracy</th>\n",
              "      <th>Training Time (sec.)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>EfficientNetB0</td>\n",
              "      <td>0.890060</td>\n",
              "      <td>177.304018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>DenseNet201</td>\n",
              "      <td>0.887321</td>\n",
              "      <td>451.696047</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>EfficientNetB5</td>\n",
              "      <td>0.886250</td>\n",
              "      <td>574.599987</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>DenseNet169</td>\n",
              "      <td>0.884524</td>\n",
              "      <td>306.946471</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>EfficientNetB7</td>\n",
              "      <td>0.882321</td>\n",
              "      <td>998.954480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>ResNet152</td>\n",
              "      <td>0.881548</td>\n",
              "      <td>687.762060</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>EfficientNetB3</td>\n",
              "      <td>0.880833</td>\n",
              "      <td>330.666761</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>MobileNet</td>\n",
              "      <td>0.879940</td>\n",
              "      <td>203.467785</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>EfficientNetB1</td>\n",
              "      <td>0.879345</td>\n",
              "      <td>225.239518</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>EfficientNetB4</td>\n",
              "      <td>0.877798</td>\n",
              "      <td>452.449111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>EfficientNetB2</td>\n",
              "      <td>0.869048</td>\n",
              "      <td>269.543511</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>MobileNetV2</td>\n",
              "      <td>0.859762</td>\n",
              "      <td>167.207469</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>ResNet101</td>\n",
              "      <td>0.839762</td>\n",
              "      <td>507.354359</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>ResNet50</td>\n",
              "      <td>0.838988</td>\n",
              "      <td>324.750563</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>ResNet152V2</td>\n",
              "      <td>0.837679</td>\n",
              "      <td>604.879852</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>ResNet50V2</td>\n",
              "      <td>0.837381</td>\n",
              "      <td>264.664696</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>EfficientNetB6</td>\n",
              "      <td>0.832917</td>\n",
              "      <td>713.165540</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>VGG16</td>\n",
              "      <td>0.830298</td>\n",
              "      <td>395.560018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>ResNet101V2</td>\n",
              "      <td>0.819940</td>\n",
              "      <td>447.922633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>DenseNet121</td>\n",
              "      <td>0.811429</td>\n",
              "      <td>264.583645</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>Xception</td>\n",
              "      <td>0.801429</td>\n",
              "      <td>398.183414</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>VGG19</td>\n",
              "      <td>0.796607</td>\n",
              "      <td>446.267296</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>InceptionV3</td>\n",
              "      <td>0.763274</td>\n",
              "      <td>206.191503</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>InceptionResNetV2</td>\n",
              "      <td>0.751071</td>\n",
              "      <td>453.456862</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>NASNetMobile</td>\n",
              "      <td>0.730655</td>\n",
              "      <td>201.932139</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                Model  Validation Accuracy  Training Time (sec.)\n",
              "0      EfficientNetB0             0.890060            177.304018\n",
              "1         DenseNet201             0.887321            451.696047\n",
              "2      EfficientNetB5             0.886250            574.599987\n",
              "3         DenseNet169             0.884524            306.946471\n",
              "4      EfficientNetB7             0.882321            998.954480\n",
              "5           ResNet152             0.881548            687.762060\n",
              "6      EfficientNetB3             0.880833            330.666761\n",
              "7           MobileNet             0.879940            203.467785\n",
              "8      EfficientNetB1             0.879345            225.239518\n",
              "9      EfficientNetB4             0.877798            452.449111\n",
              "10     EfficientNetB2             0.869048            269.543511\n",
              "11        MobileNetV2             0.859762            167.207469\n",
              "12          ResNet101             0.839762            507.354359\n",
              "13           ResNet50             0.838988            324.750563\n",
              "14        ResNet152V2             0.837679            604.879852\n",
              "15         ResNet50V2             0.837381            264.664696\n",
              "16     EfficientNetB6             0.832917            713.165540\n",
              "17              VGG16             0.830298            395.560018\n",
              "18        ResNet101V2             0.819940            447.922633\n",
              "19        DenseNet121             0.811429            264.583645\n",
              "20           Xception             0.801429            398.183414\n",
              "21              VGG19             0.796607            446.267296\n",
              "22        InceptionV3             0.763274            206.191503\n",
              "23  InceptionResNetV2             0.751071            453.456862\n",
              "24       NASNetMobile             0.730655            201.932139"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    }
  ]
}