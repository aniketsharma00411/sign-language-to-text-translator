{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "asl_transfer_learning_evaluation_data_augmentation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aniketsharma00411/sign-language-to-text-translator/blob/main/transfer_learning_evaluation_data_augmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2SPnZg5D_kx"
      },
      "source": [
        "# Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9Xgk6PSD_ky"
      },
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import applications\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "ZiMkfWayD_k0",
        "outputId": "de941702-9013-4472-a2e7-0c8aff2de5bf"
      },
      "source": [
        "if not os.path.exists(os.path.expanduser('~')+'/.kaggle'):\n",
        "    ! mkdir ~/.kaggle\n",
        "os.chdir(os.path.expanduser('~')+'/.kaggle')\n",
        "if not os.path.exists(os.path.expanduser('~')+'/.kaggle/kaggle.json'):\n",
        "    kaggle_api_file = files.upload()\n",
        "    ! kaggle datasets download -d grassknoted/asl-alphabet\n",
        "    ! unzip -q asl-alphabet.zip\n",
        "    ! rm -rf asl_alphabet_train/asl_alphabet_train/del"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-618ea674-362a-4218-9e5d-728bcf036a1c\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-618ea674-362a-4218-9e5d-728bcf036a1c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "Downloading asl-alphabet.zip to /root/.kaggle\n",
            " 98% 1.01G/1.03G [00:05<00:00, 201MB/s]\n",
            "100% 1.03G/1.03G [00:05<00:00, 196MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8dgYLreFq_w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18580523-6b85-4ad8-b149-7bd9c7dd4116"
      },
      "source": [
        "! ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "asl_alphabet_test  asl_alphabet_train  asl-alphabet.zip  kaggle.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sytb1W-rzd2"
      },
      "source": [
        "# Creating Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDN0CdRz4rDR"
      },
      "source": [
        "def get_data(preprocessing_function):\n",
        "    image_gen = ImageDataGenerator(rotation_range=20,\n",
        "                                   width_shift_range=0.1,\n",
        "                                   height_shift_range=0.1,\n",
        "                                   brightness_range=(0.2, 1),\n",
        "                                   shear_range=45,\n",
        "                                   zoom_range=(0.5, 1.5),\n",
        "                                   fill_mode='reflect',\n",
        "                                   horizontal_flip=True,\n",
        "                                   preprocessing_function=preprocessing_function,\n",
        "                                   validation_split=0.2)\n",
        "\n",
        "    train_data = 'asl_alphabet_train/asl_alphabet_train'\n",
        "\n",
        "    train_gen = image_gen.flow_from_directory(train_data,\n",
        "                                              target_size=(224, 224),\n",
        "                                              class_mode='categorical',\n",
        "                                              color_mode='rgb',\n",
        "                                              shuffle=True,\n",
        "                                              batch_size=128,\n",
        "                                              seed=0,\n",
        "                                              subset='training')\n",
        "\n",
        "    val_gen = image_gen.flow_from_directory(train_data,\n",
        "                                            target_size=(224, 224),\n",
        "                                            class_mode='categorical',\n",
        "                                            color_mode='rgb',\n",
        "                                            shuffle=True,\n",
        "                                            batch_size=128,\n",
        "                                            seed=0,\n",
        "                                            subset='validation')\n",
        "\n",
        "    return train_gen, val_gen"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvum5L5QD_k7"
      },
      "source": [
        "def get_model(model):\n",
        "    kwargs =    {'include_top':False,\n",
        "                 'weights':'imagenet',\n",
        "                 'input_shape':(224, 224, 3),\n",
        "                 'pooling':'avg'}\n",
        "    \n",
        "    base_model = model(**kwargs)\n",
        "    \n",
        "    end_model = models.Sequential()\n",
        "    end_model.add(layers.Flatten(input_shape=base_model.output_shape[1:]))\n",
        "    end_model.add(layers.Dense(64))\n",
        "    end_model.add(layers.LeakyReLU())\n",
        "    end_model.add(layers.Dense(64))\n",
        "    end_model.add(layers.LeakyReLU())\n",
        "    end_model.add(layers.Dense(28, activation='softmax'))\n",
        "\n",
        "    model = models.Model(inputs=base_model.input, outputs=end_model(base_model.output))\n",
        "\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSE1wHcbD_k7"
      },
      "source": [
        "models_dict = {\n",
        "    'Xception': {'model': applications.xception.Xception,\n",
        "                 'preprocess_func': applications.xception.preprocess_input},\n",
        "    'VGG16': {'model': applications.vgg16.VGG16,\n",
        "              'preprocess_func': applications.vgg16.preprocess_input},\n",
        "    'VGG19': {'model': applications.vgg19.VGG19,\n",
        "              'preprocess_func': applications.vgg19.preprocess_input},\n",
        "    'ResNet50': {'model': applications.resnet.ResNet50,\n",
        "                 'preprocess_func': applications.resnet.preprocess_input},\n",
        "    'ResNet101': {'model': applications.resnet.ResNet101,\n",
        "                  'preprocess_func': applications.resnet.preprocess_input},\n",
        "    'ResNet152': {'model': applications.resnet.ResNet152,\n",
        "                  'preprocess_func': applications.resnet.preprocess_input},\n",
        "    'ResNet50V2': {'model': applications.resnet_v2.ResNet50V2,\n",
        "                   'preprocess_func': applications.resnet_v2.preprocess_input},\n",
        "    'ResNet101V2': {'model': applications.resnet_v2.ResNet101V2,\n",
        "                    'preprocess_func': applications.resnet_v2.preprocess_input},\n",
        "    'ResNet152V2': {'model': applications.resnet_v2.ResNet152V2,\n",
        "                    'preprocess_func': applications.resnet_v2.preprocess_input},\n",
        "    'InceptionV3': {'model': applications.inception_v3.InceptionV3,\n",
        "                    'preprocess_func': applications.inception_v3.preprocess_input},\n",
        "    'InceptionResNetV2': {'model': applications.inception_resnet_v2.InceptionResNetV2,\n",
        "                          'preprocess_func': applications.inception_resnet_v2.preprocess_input},\n",
        "    'MobileNet': {'model': applications.mobilenet.MobileNet,\n",
        "                  'preprocess_func': applications.mobilenet.preprocess_input},\n",
        "    'MobileNetV2': {'model': applications.mobilenet_v2.MobileNetV2,\n",
        "                    'preprocess_func': applications.mobilenet_v2.preprocess_input},\n",
        "    'DenseNet121': {'model': applications.densenet.DenseNet121,\n",
        "                    'preprocess_func': applications.densenet.preprocess_input},\n",
        "    'DenseNet169': {'model': applications.densenet.DenseNet169,\n",
        "                    'preprocess_func': applications.densenet.preprocess_input},\n",
        "    'DenseNet201': {'model': applications.densenet.DenseNet201,\n",
        "                    'preprocess_func': applications.densenet.preprocess_input},\n",
        "    'NASNetMobile': {'model': applications.nasnet.NASNetMobile,\n",
        "                     'preprocess_func': applications.nasnet.preprocess_input},\n",
        "    'EfficientNetB0': {'model': applications.efficientnet.EfficientNetB0,\n",
        "                       'preprocess_func': applications.efficientnet.preprocess_input},\n",
        "    'EfficientNetB1': {'model': applications.efficientnet.EfficientNetB1,\n",
        "                       'preprocess_func': applications.efficientnet.preprocess_input},\n",
        "    'EfficientNetB2': {'model': applications.efficientnet.EfficientNetB2,\n",
        "                       'preprocess_func': applications.efficientnet.preprocess_input},\n",
        "    'EfficientNetB3': {'model': applications.efficientnet.EfficientNetB3,\n",
        "                       'preprocess_func': applications.efficientnet.preprocess_input},\n",
        "    'EfficientNetB4': {'model': applications.efficientnet.EfficientNetB4,\n",
        "                       'preprocess_func': applications.efficientnet.preprocess_input},\n",
        "    'EfficientNetB5': {'model': applications.efficientnet.EfficientNetB5,\n",
        "                       'preprocess_func': applications.efficientnet.preprocess_input},\n",
        "    'EfficientNetB6': {'model': applications.efficientnet.EfficientNetB6,\n",
        "                       'preprocess_func': applications.efficientnet.preprocess_input},\n",
        "    'EfficientNetB7': {'model': applications.efficientnet.EfficientNetB7,\n",
        "                       'preprocess_func': applications.efficientnet.preprocess_input}\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3zxUdPJD_k7"
      },
      "source": [
        "# Training Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6f2HJeZqtIK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd89b581-e2c7-48fe-8650-63499f83b454"
      },
      "source": [
        "for name, model in models_dict.items():\n",
        "    print(name)\n",
        "\n",
        "    train_gen, val_gen = get_data(model['preprocess_func'])\n",
        "    m = get_model(model['model'])\n",
        "\n",
        "    start = time.perf_counter()\n",
        "\n",
        "    history = m.fit(train_gen,\n",
        "                    epochs=1,\n",
        "                    validation_data=val_gen)\n",
        "\n",
        "    duration = time.perf_counter() - start\n",
        "\n",
        "    models_dict[name]['time'] = duration\n",
        "    models_dict[name]['val_acc'] = history.history['val_accuracy'][-1]\n",
        "    \n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Xception\n",
            "Found 67200 images belonging to 28 classes.\n",
            "Found 16800 images belonging to 28 classes.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "83689472/83683744 [==============================] - 1s 0us/step\n",
            "525/525 [==============================] - 1056s 2s/step - loss: 1.6490 - accuracy: 0.5095 - val_loss: 1.6198 - val_accuracy: 0.5070\n",
            "\n",
            "VGG16\n",
            "Found 67200 images belonging to 28 classes.\n",
            "Found 16800 images belonging to 28 classes.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 0s 0us/step\n",
            "525/525 [==============================] - 1079s 2s/step - loss: 1.7234 - accuracy: 0.4907 - val_loss: 1.5898 - val_accuracy: 0.5186\n",
            "\n",
            "VGG19\n",
            "Found 67200 images belonging to 28 classes.\n",
            "Found 16800 images belonging to 28 classes.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "80142336/80134624 [==============================] - 1s 0us/step\n",
            "525/525 [==============================] - 1084s 2s/step - loss: 1.7223 - accuracy: 0.4912 - val_loss: 1.6270 - val_accuracy: 0.5135\n",
            "\n",
            "ResNet50\n",
            "Found 67200 images belonging to 28 classes.\n",
            "Found 16800 images belonging to 28 classes.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94773248/94765736 [==============================] - 0s 0us/step\n",
            "525/525 [==============================] - 1032s 2s/step - loss: 1.3508 - accuracy: 0.5900 - val_loss: 1.2841 - val_accuracy: 0.5951\n",
            "\n",
            "ResNet101\n",
            "Found 67200 images belonging to 28 classes.\n",
            "Found 16800 images belonging to 28 classes.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet101_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "171450368/171446536 [==============================] - 1s 0us/step\n",
            "525/525 [==============================] - 1098s 2s/step - loss: 1.3909 - accuracy: 0.5804 - val_loss: 1.2632 - val_accuracy: 0.6101\n",
            "\n",
            "ResNet152\n",
            "Found 67200 images belonging to 28 classes.\n",
            "Found 16800 images belonging to 28 classes.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet152_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "234700800/234698864 [==============================] - 2s 0us/step\n",
            "525/525 [==============================] - 1176s 2s/step - loss: 1.3042 - accuracy: 0.6032 - val_loss: 1.2176 - val_accuracy: 0.6262\n",
            "\n",
            "ResNet50V2\n",
            "Found 67200 images belonging to 28 classes.\n",
            "Found 16800 images belonging to 28 classes.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94674944/94668760 [==============================] - 1s 0us/step\n",
            "525/525 [==============================] - 986s 2s/step - loss: 1.2701 - accuracy: 0.6150 - val_loss: 1.3393 - val_accuracy: 0.5926\n",
            "\n",
            "ResNet101V2\n",
            "Found 67200 images belonging to 28 classes.\n",
            "Found 16800 images belonging to 28 classes.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet101v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "171319296/171317808 [==============================] - 1s 0us/step\n",
            "525/525 [==============================] - 1055s 2s/step - loss: 1.2641 - accuracy: 0.6183 - val_loss: 1.3055 - val_accuracy: 0.6046\n",
            "\n",
            "ResNet152V2\n",
            "Found 67200 images belonging to 28 classes.\n",
            "Found 16800 images belonging to 28 classes.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet152v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "234553344/234545216 [==============================] - 1s 0us/step\n",
            "525/525 [==============================] - 1129s 2s/step - loss: 1.3004 - accuracy: 0.6114 - val_loss: 1.3645 - val_accuracy: 0.5836\n",
            "\n",
            "InceptionV3\n",
            "Found 67200 images belonging to 28 classes.\n",
            "Found 16800 images belonging to 28 classes.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87916544/87910968 [==============================] - 0s 0us/step\n",
            "525/525 [==============================] - 964s 2s/step - loss: 1.7322 - accuracy: 0.4829 - val_loss: 1.7942 - val_accuracy: 0.4631\n",
            "\n",
            "InceptionResNetV2\n",
            "Found 67200 images belonging to 28 classes.\n",
            "Found 16800 images belonging to 28 classes.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_resnet_v2/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "219062272/219055592 [==============================] - 1s 0us/step\n",
            "525/525 [==============================] - 1061s 2s/step - loss: 1.7607 - accuracy: 0.4714 - val_loss: 1.6960 - val_accuracy: 0.4835\n",
            "\n",
            "MobileNet\n",
            "Found 67200 images belonging to 28 classes.\n",
            "Found 16800 images belonging to 28 classes.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet/mobilenet_1_0_224_tf_no_top.h5\n",
            "17227776/17225924 [==============================] - 0s 0us/step\n",
            "525/525 [==============================] - 932s 2s/step - loss: 1.2293 - accuracy: 0.6334 - val_loss: 1.1099 - val_accuracy: 0.6595\n",
            "\n",
            "MobileNetV2\n",
            "Found 67200 images belonging to 28 classes.\n",
            "Found 16800 images belonging to 28 classes.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "9412608/9406464 [==============================] - 0s 0us/step\n",
            "525/525 [==============================] - 932s 2s/step - loss: 1.5625 - accuracy: 0.5386 - val_loss: 1.5753 - val_accuracy: 0.5228\n",
            "\n",
            "DenseNet121\n",
            "Found 67200 images belonging to 28 classes.\n",
            "Found 16800 images belonging to 28 classes.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "29089792/29084464 [==============================] - 0s 0us/step\n",
            "525/525 [==============================] - 1014s 2s/step - loss: 1.6044 - accuracy: 0.5246 - val_loss: 1.6054 - val_accuracy: 0.5092\n",
            "\n",
            "DenseNet169\n",
            "Found 67200 images belonging to 28 classes.\n",
            "Found 16800 images belonging to 28 classes.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet169_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "51879936/51877672 [==============================] - 0s 0us/step\n",
            "525/525 [==============================] - 1026s 2s/step - loss: 1.4248 - accuracy: 0.5769 - val_loss: 1.2999 - val_accuracy: 0.5943\n",
            "\n",
            "DenseNet201\n",
            "Found 67200 images belonging to 28 classes.\n",
            "Found 16800 images belonging to 28 classes.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet201_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "74842112/74836368 [==============================] - 0s 0us/step\n",
            "525/525 [==============================] - 1066s 2s/step - loss: 1.4432 - accuracy: 0.5761 - val_loss: 1.2495 - val_accuracy: 0.6149\n",
            "\n",
            "NASNetMobile\n",
            "Found 67200 images belonging to 28 classes.\n",
            "Found 16800 images belonging to 28 classes.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/nasnet/NASNet-mobile-no-top.h5\n",
            "19996672/19993432 [==============================] - 0s 0us/step\n",
            "525/525 [==============================] - 980s 2s/step - loss: 1.8803 - accuracy: 0.4400 - val_loss: 1.8928 - val_accuracy: 0.4338\n",
            "\n",
            "EfficientNetB0\n",
            "Found 67200 images belonging to 28 classes.\n",
            "Found 16800 images belonging to 28 classes.\n",
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
            "16711680/16705208 [==============================] - 0s 0us/step\n",
            "525/525 [==============================] - 958s 2s/step - loss: 1.5064 - accuracy: 0.5592 - val_loss: 1.4044 - val_accuracy: 0.5838\n",
            "\n",
            "EfficientNetB1\n",
            "Found 67200 images belonging to 28 classes.\n",
            "Found 16800 images belonging to 28 classes.\n",
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb1_notop.h5\n",
            "27025408/27018416 [==============================] - 1s 0us/step\n",
            "525/525 [==============================] - 968s 2s/step - loss: 1.4626 - accuracy: 0.5682 - val_loss: 1.2998 - val_accuracy: 0.6018\n",
            "\n",
            "EfficientNetB2\n",
            "Found 67200 images belonging to 28 classes.\n",
            "Found 16800 images belonging to 28 classes.\n",
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb2_notop.h5\n",
            "31793152/31790344 [==============================] - 0s 0us/step\n",
            "525/525 [==============================] - 971s 2s/step - loss: 1.5177 - accuracy: 0.5515 - val_loss: 1.4236 - val_accuracy: 0.5710\n",
            "\n",
            "EfficientNetB3\n",
            "Found 67200 images belonging to 28 classes.\n",
            "Found 16800 images belonging to 28 classes.\n",
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb3_notop.h5\n",
            "43941888/43941136 [==============================] - 0s 0us/step\n",
            "525/525 [==============================] - 997s 2s/step - loss: 1.4744 - accuracy: 0.5643 - val_loss: 1.3629 - val_accuracy: 0.5872\n",
            "\n",
            "EfficientNetB4\n",
            "Found 67200 images belonging to 28 classes.\n",
            "Found 16800 images belonging to 28 classes.\n",
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb4_notop.h5\n",
            "71688192/71686520 [==============================] - 0s 0us/step\n",
            "525/525 [==============================] - 1031s 2s/step - loss: 1.5659 - accuracy: 0.5386 - val_loss: 1.4692 - val_accuracy: 0.5595\n",
            "\n",
            "EfficientNetB5\n",
            "Found 67200 images belonging to 28 classes.\n",
            "Found 16800 images belonging to 28 classes.\n",
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb5_notop.h5\n",
            "115269632/115263384 [==============================] - 1s 0us/step\n",
            "525/525 [==============================] - 1081s 2s/step - loss: 1.3631 - accuracy: 0.6018 - val_loss: 1.2469 - val_accuracy: 0.6257\n",
            "\n",
            "EfficientNetB6\n",
            "Found 67200 images belonging to 28 classes.\n",
            "Found 16800 images belonging to 28 classes.\n",
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb6_notop.h5\n",
            "165240832/165234480 [==============================] - 1s 0us/step\n",
            "525/525 [==============================] - 1157s 2s/step - loss: 1.5887 - accuracy: 0.5277 - val_loss: 1.4628 - val_accuracy: 0.5565\n",
            "\n",
            "EfficientNetB7\n",
            "Found 67200 images belonging to 28 classes.\n",
            "Found 16800 images belonging to 28 classes.\n",
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb7_notop.h5\n",
            "258080768/258076736 [==============================] - 1s 0us/step\n",
            "525/525 [==============================] - 1250s 2s/step - loss: 1.3087 - accuracy: 0.6098 - val_loss: 1.2865 - val_accuracy: 0.6169\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3INO9J8r8u_"
      },
      "source": [
        "# Evaluating and Visualizing results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gga43tnFD_k8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 834
        },
        "outputId": "2b5871de-d2dc-48e0-ad0e-296b4d554486"
      },
      "source": [
        "model_results = []\n",
        "\n",
        "for name, _ in models_dict.items():\n",
        "    model_results.append([name,\n",
        "                          models_dict[name]['val_acc'],\n",
        "                          models_dict[name]['time']])\n",
        "    \n",
        "results = pd.DataFrame(model_results,\n",
        "                       columns = ['Model', 'Validation Accuracy', 'Training Time (sec.)'])\n",
        "results = results.sort_values(by='Validation Accuracy', ascending=False).reset_index(drop=True)\n",
        "\n",
        "results"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Validation Accuracy</th>\n",
              "      <th>Training Time (sec.)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>MobileNet</td>\n",
              "      <td>0.659464</td>\n",
              "      <td>933.186254</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ResNet152</td>\n",
              "      <td>0.626190</td>\n",
              "      <td>1177.023145</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>EfficientNetB5</td>\n",
              "      <td>0.625714</td>\n",
              "      <td>1082.315889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>EfficientNetB7</td>\n",
              "      <td>0.616905</td>\n",
              "      <td>1251.437560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>DenseNet201</td>\n",
              "      <td>0.614940</td>\n",
              "      <td>1067.191720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>ResNet101</td>\n",
              "      <td>0.610059</td>\n",
              "      <td>1099.715862</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>ResNet101V2</td>\n",
              "      <td>0.604643</td>\n",
              "      <td>1056.137887</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>EfficientNetB1</td>\n",
              "      <td>0.601786</td>\n",
              "      <td>969.576498</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>ResNet50</td>\n",
              "      <td>0.595119</td>\n",
              "      <td>1033.249712</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>DenseNet169</td>\n",
              "      <td>0.594345</td>\n",
              "      <td>1027.773288</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>ResNet50V2</td>\n",
              "      <td>0.592619</td>\n",
              "      <td>987.461330</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>EfficientNetB3</td>\n",
              "      <td>0.587202</td>\n",
              "      <td>1050.683825</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>EfficientNetB0</td>\n",
              "      <td>0.583809</td>\n",
              "      <td>959.435314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>ResNet152V2</td>\n",
              "      <td>0.583631</td>\n",
              "      <td>1170.538862</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>EfficientNetB2</td>\n",
              "      <td>0.570952</td>\n",
              "      <td>972.182747</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>EfficientNetB4</td>\n",
              "      <td>0.559464</td>\n",
              "      <td>1032.165502</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>EfficientNetB6</td>\n",
              "      <td>0.556488</td>\n",
              "      <td>1176.328282</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>MobileNetV2</td>\n",
              "      <td>0.522798</td>\n",
              "      <td>933.126028</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>VGG16</td>\n",
              "      <td>0.518631</td>\n",
              "      <td>1080.140307</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>VGG19</td>\n",
              "      <td>0.513512</td>\n",
              "      <td>1085.934354</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>DenseNet121</td>\n",
              "      <td>0.509167</td>\n",
              "      <td>1015.046580</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>Xception</td>\n",
              "      <td>0.506964</td>\n",
              "      <td>1057.801588</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>InceptionResNetV2</td>\n",
              "      <td>0.483452</td>\n",
              "      <td>1062.545595</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>InceptionV3</td>\n",
              "      <td>0.463095</td>\n",
              "      <td>965.605883</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>NASNetMobile</td>\n",
              "      <td>0.433810</td>\n",
              "      <td>981.754215</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                Model  Validation Accuracy  Training Time (sec.)\n",
              "0           MobileNet             0.659464            933.186254\n",
              "1           ResNet152             0.626190           1177.023145\n",
              "2      EfficientNetB5             0.625714           1082.315889\n",
              "3      EfficientNetB7             0.616905           1251.437560\n",
              "4         DenseNet201             0.614940           1067.191720\n",
              "5           ResNet101             0.610059           1099.715862\n",
              "6         ResNet101V2             0.604643           1056.137887\n",
              "7      EfficientNetB1             0.601786            969.576498\n",
              "8            ResNet50             0.595119           1033.249712\n",
              "9         DenseNet169             0.594345           1027.773288\n",
              "10         ResNet50V2             0.592619            987.461330\n",
              "11     EfficientNetB3             0.587202           1050.683825\n",
              "12     EfficientNetB0             0.583809            959.435314\n",
              "13        ResNet152V2             0.583631           1170.538862\n",
              "14     EfficientNetB2             0.570952            972.182747\n",
              "15     EfficientNetB4             0.559464           1032.165502\n",
              "16     EfficientNetB6             0.556488           1176.328282\n",
              "17        MobileNetV2             0.522798            933.126028\n",
              "18              VGG16             0.518631           1080.140307\n",
              "19              VGG19             0.513512           1085.934354\n",
              "20        DenseNet121             0.509167           1015.046580\n",
              "21           Xception             0.506964           1057.801588\n",
              "22  InceptionResNetV2             0.483452           1062.545595\n",
              "23        InceptionV3             0.463095            965.605883\n",
              "24       NASNetMobile             0.433810            981.754215"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    }
  ]
}